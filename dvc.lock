schema: '2.0'
stages:
  format_datasets:
    cmd: python -m src.stages.format_datasets --config=params.yaml
    deps:
    - path: ./datasets/raw/
      hash: md5
      md5: dc007414179c80b25fa562388134de22.dir
      size: 277862800
      nfiles: 194
    - path: src/stages/format_datasets.py
      hash: md5
      md5: 60e286109cc57530df5a9d6abc744ed1
      size: 2513
    params:
      params.yaml:
        base:
          project_name: correctness-model-internals
          log_level: DEBUG
          models_dir: ../models
          datasets_dir: ./datasets
          generations_dir: ./generations
          evaluations_dir: ./evaluations
          activations_dir: ./activations
          classification_stats_dir: ./classification_stats
        datasets:
          cities_10k:
            answer_type: open_ended
            subsets:
            - main
            format: csv
            eval_type: answers_map
            max_new_tokens: 128
            stop_words:
            - <|eot_id|>
            - "\n"
            answers_map_path: eval_map.json
            col_map:
              prompt: question
              answer: correct_answer
            prompts:
              base: "I am going to ask you a question about cities. End your sentence
                with <|eot_id|>.\nHere are some examples of questions that might help
                you:\n---\nQuestion: In which country is the city of Barcelona located?\n
                Answer: Spain<|eot_id|>\n---\nQuestion: In which country is the city
                of Berlin located?\nAnswer: Germany<|eot_id|>\n---\nQuestion: In which
                country is the city of Beijing located?\nAnswer: China<|eot_id|>\n\
                ---\nQuestion:"
          birth_years_4k:
            answer_type: open_ended
            subsets:
            - main
            format: csv
            eval_type: exact_match
            max_new_tokens: 128
            stop_words:
            - <|eot_id|>
            - "\n"
            col_map:
              prompt: question
              answer: correct_answer
            prompts:
              base: "I am going to ask you what year a person was born. End your sentence
                with <|eot_id|>.\nHere are some examples of questions that might help
                you:\n---\nQuestion: What year was Barack Obama born? \nAnswer: 1961<|eot_id|>\n
                ---\nQuestion: What year was Vladimir Putin born? \nAnswer: 1952<|eot_id|>\n
                ---\nQuestion: What year was Xi Jinping born? \nAnswer: 1953<|eot_id|>\n\
                ---\nQuestion:"
          medals_9k:
            answer_type: open_ended
            subsets:
            - main
            format: csv
            eval_type: answers_map
            max_new_tokens: 128
            stop_words:
            - <|eot_id|>
            - "\n"
            answers_map_path: eval_map.json
            col_map:
              prompt: question
              answer: correct_answer
            prompts:
              base: "I am going to ask you a question about the olympics. End your
                sentence with <|eot_id|>.\nHere are some examples of questions that
                might help you:\n---\nQuestion: Which country won gold in Gymnastics
                Women's Team All-Around in the 1928 Summer Olympics?\nAnswer: Netherlands<|eot_id|>
                \n---\nQuestion: Which country won gold in Hockey Women's Hockey in
                the 2004 Summer Olympics? Answer: Germany<|eot_id|> \n---\nQuestion:
                Which country won gold in Fencing Men's Sabre, Individual in the 1964
                Summer Olympics?\nAnswer: Hungary<|eot_id|>\n---\nQuestion:"
          football_leagues_1k:
            answer_type: open_ended
            subsets:
            - main
            format: csv
            eval_type: answers_map
            max_new_tokens: 128
            stop_words:
            - <|eot_id|>
            - "\n"
            answers_map_path: eval_map.json
            col_map:
              prompt: question
              answer: correct_answer
            prompts:
              base: "I am going to ask you a question about football. End your sentence
                with <|eot_id|>.\nHere are some examples of questions that might help
                you:\n---\nQuestion: Which team finished in position 10 in the 2010-2011
                Premier League?\nAnswer: Sunderland<|eot_id|> \n---\nQuestion: Which
                team finished in position 19 in the 2020-2021 La Liga?\nAnswer: Valladolid<|eot_id|>
                \n---\nQuestion: Which team finished in position 13 in the 2019-2020
                Serie A?\nAnswer: Udinese<|eot_id|>\n---\nQuestion:"
        format_datasets:
          raw_dir_path: raw
          formatted_dir_path: formatted
          generation_delimiter: 'Answer:'
    outs:
    - path: ./datasets/formatted/
      hash: md5
      md5: 2bb0047873630b15d3e4b58692ad76ea.dir
      size: 13142642
      nfiles: 4
  generate_answers:
    cmd: python -m src.stages.generate_answers --config=params.yaml --model=llama3.1_8b_chat
      --batch-size=10
    deps:
    - path: ./datasets/formatted/
      hash: md5
      md5: 2bb0047873630b15d3e4b58692ad76ea.dir
      size: 13142642
      nfiles: 4
    - path: src/stages/generate_answers.py
      hash: md5
      md5: 8b4e65e9887f7411f50060d500b99b99
      size: 9272
    params:
      params.yaml:
        base:
          project_name: correctness-model-internals
          log_level: DEBUG
          models_dir: ../models
          datasets_dir: ./datasets
          generations_dir: ./generations
          evaluations_dir: ./evaluations
          activations_dir: ./activations
          classification_stats_dir: ./classification_stats
        datasets:
          cities_10k:
            answer_type: open_ended
            subsets:
            - main
            format: csv
            eval_type: answers_map
            max_new_tokens: 128
            stop_words:
            - <|eot_id|>
            - "\n"
            answers_map_path: eval_map.json
            col_map:
              prompt: question
              answer: correct_answer
            prompts:
              base: "I am going to ask you a question about cities. End your sentence
                with <|eot_id|>.\nHere are some examples of questions that might help
                you:\n---\nQuestion: In which country is the city of Barcelona located?\n
                Answer: Spain<|eot_id|>\n---\nQuestion: In which country is the city
                of Berlin located?\nAnswer: Germany<|eot_id|>\n---\nQuestion: In which
                country is the city of Beijing located?\nAnswer: China<|eot_id|>\n\
                ---\nQuestion:"
          birth_years_4k:
            answer_type: open_ended
            subsets:
            - main
            format: csv
            eval_type: exact_match
            max_new_tokens: 128
            stop_words:
            - <|eot_id|>
            - "\n"
            col_map:
              prompt: question
              answer: correct_answer
            prompts:
              base: "I am going to ask you what year a person was born. End your sentence
                with <|eot_id|>.\nHere are some examples of questions that might help
                you:\n---\nQuestion: What year was Barack Obama born? \nAnswer: 1961<|eot_id|>\n
                ---\nQuestion: What year was Vladimir Putin born? \nAnswer: 1952<|eot_id|>\n
                ---\nQuestion: What year was Xi Jinping born? \nAnswer: 1953<|eot_id|>\n\
                ---\nQuestion:"
          medals_9k:
            answer_type: open_ended
            subsets:
            - main
            format: csv
            eval_type: answers_map
            max_new_tokens: 128
            stop_words:
            - <|eot_id|>
            - "\n"
            answers_map_path: eval_map.json
            col_map:
              prompt: question
              answer: correct_answer
            prompts:
              base: "I am going to ask you a question about the olympics. End your
                sentence with <|eot_id|>.\nHere are some examples of questions that
                might help you:\n---\nQuestion: Which country won gold in Gymnastics
                Women's Team All-Around in the 1928 Summer Olympics?\nAnswer: Netherlands<|eot_id|>
                \n---\nQuestion: Which country won gold in Hockey Women's Hockey in
                the 2004 Summer Olympics? Answer: Germany<|eot_id|> \n---\nQuestion:
                Which country won gold in Fencing Men's Sabre, Individual in the 1964
                Summer Olympics?\nAnswer: Hungary<|eot_id|>\n---\nQuestion:"
          football_leagues_1k:
            answer_type: open_ended
            subsets:
            - main
            format: csv
            eval_type: answers_map
            max_new_tokens: 128
            stop_words:
            - <|eot_id|>
            - "\n"
            answers_map_path: eval_map.json
            col_map:
              prompt: question
              answer: correct_answer
            prompts:
              base: "I am going to ask you a question about football. End your sentence
                with <|eot_id|>.\nHere are some examples of questions that might help
                you:\n---\nQuestion: Which team finished in position 10 in the 2010-2011
                Premier League?\nAnswer: Sunderland<|eot_id|> \n---\nQuestion: Which
                team finished in position 19 in the 2020-2021 La Liga?\nAnswer: Valladolid<|eot_id|>
                \n---\nQuestion: Which team finished in position 13 in the 2019-2020
                Serie A?\nAnswer: Udinese<|eot_id|>\n---\nQuestion:"
        format_datasets:
          raw_dir_path: raw
          formatted_dir_path: formatted
          generation_delimiter: 'Answer:'
        generate_answers:
          model: llama3.1_8b_chat
          max_dataset_size: false
          sample_strategy: first_n
          batch_size: 10
          inference_engine: vllm
    outs:
    - path: ./generations/
      hash: md5
      md5: 717740529d58a74a9d0b7d1dfaaa14f8.dir
      size: 13148563
      nfiles: 4
  evaluate_answers:
    cmd: python -m src.stages.evaluate_answers --config=params.yaml --model=llama3.1_8b_chat
    deps:
    - path: ./datasets/formatted/
      hash: md5
      md5: 2bb0047873630b15d3e4b58692ad76ea.dir
      size: 13142642
      nfiles: 4
    - path: ./generations/
      hash: md5
      md5: 717740529d58a74a9d0b7d1dfaaa14f8.dir
      size: 13148563
      nfiles: 4
    - path: src/stages/evaluate_answers.py
      hash: md5
      md5: a3bca029e24153f2840bf0c87e12d608
      size: 8544
    params:
      params.yaml:
        base:
          project_name: correctness-model-internals
          log_level: DEBUG
          models_dir: ../models
          datasets_dir: ./datasets
          generations_dir: ./generations
          evaluations_dir: ./evaluations
          activations_dir: ./activations
          classification_stats_dir: ./classification_stats
        datasets:
          cities_10k:
            answer_type: open_ended
            subsets:
            - main
            format: csv
            eval_type: answers_map
            max_new_tokens: 128
            stop_words:
            - <|eot_id|>
            - "\n"
            answers_map_path: eval_map.json
            col_map:
              prompt: question
              answer: correct_answer
            prompts:
              base: "I am going to ask you a question about cities. End your sentence
                with <|eot_id|>.\nHere are some examples of questions that might help
                you:\n---\nQuestion: In which country is the city of Barcelona located?\n
                Answer: Spain<|eot_id|>\n---\nQuestion: In which country is the city
                of Berlin located?\nAnswer: Germany<|eot_id|>\n---\nQuestion: In which
                country is the city of Beijing located?\nAnswer: China<|eot_id|>\n\
                ---\nQuestion:"
          birth_years_4k:
            answer_type: open_ended
            subsets:
            - main
            format: csv
            eval_type: exact_match
            max_new_tokens: 128
            stop_words:
            - <|eot_id|>
            - "\n"
            col_map:
              prompt: question
              answer: correct_answer
            prompts:
              base: "I am going to ask you what year a person was born. End your sentence
                with <|eot_id|>.\nHere are some examples of questions that might help
                you:\n---\nQuestion: What year was Barack Obama born? \nAnswer: 1961<|eot_id|>\n
                ---\nQuestion: What year was Vladimir Putin born? \nAnswer: 1952<|eot_id|>\n
                ---\nQuestion: What year was Xi Jinping born? \nAnswer: 1953<|eot_id|>\n\
                ---\nQuestion:"
          medals_9k:
            answer_type: open_ended
            subsets:
            - main
            format: csv
            eval_type: answers_map
            max_new_tokens: 128
            stop_words:
            - <|eot_id|>
            - "\n"
            answers_map_path: eval_map.json
            col_map:
              prompt: question
              answer: correct_answer
            prompts:
              base: "I am going to ask you a question about the olympics. End your
                sentence with <|eot_id|>.\nHere are some examples of questions that
                might help you:\n---\nQuestion: Which country won gold in Gymnastics
                Women's Team All-Around in the 1928 Summer Olympics?\nAnswer: Netherlands<|eot_id|>
                \n---\nQuestion: Which country won gold in Hockey Women's Hockey in
                the 2004 Summer Olympics? Answer: Germany<|eot_id|> \n---\nQuestion:
                Which country won gold in Fencing Men's Sabre, Individual in the 1964
                Summer Olympics?\nAnswer: Hungary<|eot_id|>\n---\nQuestion:"
          football_leagues_1k:
            answer_type: open_ended
            subsets:
            - main
            format: csv
            eval_type: answers_map
            max_new_tokens: 128
            stop_words:
            - <|eot_id|>
            - "\n"
            answers_map_path: eval_map.json
            col_map:
              prompt: question
              answer: correct_answer
            prompts:
              base: "I am going to ask you a question about football. End your sentence
                with <|eot_id|>.\nHere are some examples of questions that might help
                you:\n---\nQuestion: Which team finished in position 10 in the 2010-2011
                Premier League?\nAnswer: Sunderland<|eot_id|> \n---\nQuestion: Which
                team finished in position 19 in the 2020-2021 La Liga?\nAnswer: Valladolid<|eot_id|>
                \n---\nQuestion: Which team finished in position 13 in the 2019-2020
                Serie A?\nAnswer: Udinese<|eot_id|>\n---\nQuestion:"
        format_datasets:
          raw_dir_path: raw
          formatted_dir_path: formatted
          generation_delimiter: 'Answer:'
        generate_answers:
          model: llama3.1_8b_chat
          max_dataset_size: false
          sample_strategy: first_n
          batch_size: 10
          inference_engine: vllm
    outs:
    - path: ./evaluations/
      hash: md5
      md5: 1cc717b9b2645e3653cf58afa1b123f5.dir
      size: 13427231
      nfiles: 9
  capture_activations:
    cmd: python -m src.stages.capture_activations --config=params.yaml --model=llama3.1_8b_chat
      --batch-size=10
    deps:
    - path: ./generations/
      hash: md5
      md5: 717740529d58a74a9d0b7d1dfaaa14f8.dir
      size: 13148563
      nfiles: 4
    - path: src/stages/capture_activations.py
      hash: md5
      md5: 74b16989da5bf2519dfef951b5777827
      size: 5734
    params:
      params.yaml:
        base:
          project_name: correctness-model-internals
          log_level: DEBUG
          models_dir: ../models
          datasets_dir: ./datasets
          generations_dir: ./generations
          evaluations_dir: ./evaluations
          activations_dir: ./activations
          classification_stats_dir: ./classification_stats
        capture_activations:
          input_type:
          - prompt_only
          - prompt_answer
        datasets:
          cities_10k:
            answer_type: open_ended
            subsets:
            - main
            format: csv
            eval_type: answers_map
            max_new_tokens: 128
            stop_words:
            - <|eot_id|>
            - "\n"
            answers_map_path: eval_map.json
            col_map:
              prompt: question
              answer: correct_answer
            prompts:
              base: "I am going to ask you a question about cities. End your sentence
                with <|eot_id|>.\nHere are some examples of questions that might help
                you:\n---\nQuestion: In which country is the city of Barcelona located?\n
                Answer: Spain<|eot_id|>\n---\nQuestion: In which country is the city
                of Berlin located?\nAnswer: Germany<|eot_id|>\n---\nQuestion: In which
                country is the city of Beijing located?\nAnswer: China<|eot_id|>\n\
                ---\nQuestion:"
          birth_years_4k:
            answer_type: open_ended
            subsets:
            - main
            format: csv
            eval_type: exact_match
            max_new_tokens: 128
            stop_words:
            - <|eot_id|>
            - "\n"
            col_map:
              prompt: question
              answer: correct_answer
            prompts:
              base: "I am going to ask you what year a person was born. End your sentence
                with <|eot_id|>.\nHere are some examples of questions that might help
                you:\n---\nQuestion: What year was Barack Obama born? \nAnswer: 1961<|eot_id|>\n
                ---\nQuestion: What year was Vladimir Putin born? \nAnswer: 1952<|eot_id|>\n
                ---\nQuestion: What year was Xi Jinping born? \nAnswer: 1953<|eot_id|>\n\
                ---\nQuestion:"
          medals_9k:
            answer_type: open_ended
            subsets:
            - main
            format: csv
            eval_type: answers_map
            max_new_tokens: 128
            stop_words:
            - <|eot_id|>
            - "\n"
            answers_map_path: eval_map.json
            col_map:
              prompt: question
              answer: correct_answer
            prompts:
              base: "I am going to ask you a question about the olympics. End your
                sentence with <|eot_id|>.\nHere are some examples of questions that
                might help you:\n---\nQuestion: Which country won gold in Gymnastics
                Women's Team All-Around in the 1928 Summer Olympics?\nAnswer: Netherlands<|eot_id|>
                \n---\nQuestion: Which country won gold in Hockey Women's Hockey in
                the 2004 Summer Olympics? Answer: Germany<|eot_id|> \n---\nQuestion:
                Which country won gold in Fencing Men's Sabre, Individual in the 1964
                Summer Olympics?\nAnswer: Hungary<|eot_id|>\n---\nQuestion:"
          football_leagues_1k:
            answer_type: open_ended
            subsets:
            - main
            format: csv
            eval_type: answers_map
            max_new_tokens: 128
            stop_words:
            - <|eot_id|>
            - "\n"
            answers_map_path: eval_map.json
            col_map:
              prompt: question
              answer: correct_answer
            prompts:
              base: "I am going to ask you a question about football. End your sentence
                with <|eot_id|>.\nHere are some examples of questions that might help
                you:\n---\nQuestion: Which team finished in position 10 in the 2010-2011
                Premier League?\nAnswer: Sunderland<|eot_id|> \n---\nQuestion: Which
                team finished in position 19 in the 2020-2021 La Liga?\nAnswer: Valladolid<|eot_id|>
                \n---\nQuestion: Which team finished in position 13 in the 2019-2020
                Serie A?\nAnswer: Udinese<|eot_id|>\n---\nQuestion:"
        generate_answers:
          model: llama3.1_8b_chat
          max_dataset_size: false
          sample_strategy: first_n
          batch_size: 10
          inference_engine: vllm
        models:
          llama3.1_8b_chat:
            dir_path: llama3.1_8b_chat
            eos_token: <\|eot_id\|>
            max_length: 1024
          llama3_8b_cot:
            dir_path: llama3_8b_cot_numina_hf
            eos_token: <\|eot_id\|>
            max_length: 1024
          llama3_3b_chat:
            dir_path: llama3_3b_chat_hf
            max_length: 1024
    outs:
    - path: ./activations/
      hash: md5
      md5: 2dce57be0338fab3b59adfb7b8e65cf8.dir
      size: 25862476032
      nfiles: 156672
