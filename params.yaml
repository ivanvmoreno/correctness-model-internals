base:
  project_name: correctness-model-internals
  log_level: DEBUG
  models_dir: ../models
  datasets_dir: ./datasets
  generations_dir: ./generations
  evaluations_dir: ./evaluations
  activations_dir: ./activations
  classification_stats_dir: ./classification_stats

datasets:
  # trivia_qa:
  #   hf_repo_id: mandarjoshi/trivia_qa
  #   answer_type: open_ended
  #   subsets:
  #     - rc
  #   prompts:
  #     base: |-
  #       I am going to ask you a question. End your sentence with <|eot_id|>.
  #       Here are some examples of questions that might help you:
  #       ---
  #       Which American-born Sinclair won the Nobel Prize for Literature in 1930?
  #       Answer: Upton Sinclair<|eot_id|> 
  #       ---
  #       Question:
  mmlu:
    hf_repo_id: cais/mmlu
    answer_type: multiple_choice
    answer_map: [A, B, C, D]
    eval_type: constrained_tokens
    subsets:
      - high_school_mathematics
      - college_mathematics
      - abstract_algebra
    prompts:
      base: The following is a multiple choice question (with answers). Give your answer with a single letter corresponding to the correct answer.
  cities_10k:
    answer_type: open_ended
    subsets:
      - main
    format: csv
    eval_type: answers_map
    max_new_tokens: 128
    stop_words: ["<|eot_id|>", "\n"]
    answers_map_path: eval_map.json
    col_map:
      prompt: question
      answer: correct_answer
    prompts:
      base: |-
        I am going to ask you a question about cities. End your sentence with <|eot_id|>.
        Here are some examples of questions that might help you:
        ---
        Question: In which country is the city of Barcelona located?
        Answer: Spain<|eot_id|>
        ---
        Question: In which country is the city of Berlin located?
        Answer: Germany<|eot_id|>
        ---
        Question: In which country is the city of Beijing located?
        Answer: China<|eot_id|>
        ---
        Question:
  birth_years_4k:
    answer_type: open_ended
    subsets:
      - main
    format: csv
    eval_type: exact_match
    max_new_tokens: 128
    stop_words: ["<|eot_id|>", "\n"]
    col_map:
      prompt: question
      answer: correct_answer
    prompts:
      base: |-
        I am going to ask you what year a person was born. End your sentence with <|eot_id|>.
        Here are some examples of questions that might help you:
        ---
        Question: What year was Barack Obama born? 
        Answer: 1961<|eot_id|>
        ---
        Question: What year was Vladimir Putin born? 
        Answer: 1952<|eot_id|>
        ---
        Question: What year was Xi Jinping born? 
        Answer: 1953<|eot_id|>
        ---
        Question:
  medals_9k:
    answer_type: open_ended
    subsets:
      - main
    format: csv
    eval_type: answers_map
    max_new_tokens: 128
    stop_words: ["<|eot_id|>", "\n"]
    answers_map_path: eval_map.json
    col_map:
      prompt: question
      answer: correct_answer
    prompts:
      base: |-
        I am going to ask you a question about the olympics. End your sentence with <|eot_id|>.
        Here are some examples of questions that might help you:
        ---
        Question: Which country won gold in Gymnastics Women's Team All-Around in the 1928 Summer Olympics?
        Answer: Netherlands<|eot_id|> 
        ---
        Question: Which country won gold in Hockey Women's Hockey in the 2004 Summer Olympics? Answer: Germany<|eot_id|> 
        ---
        Question: Which country won gold in Fencing Men's Sabre, Individual in the 1964 Summer Olympics?
        Answer: Hungary<|eot_id|>
        ---
        Question:
  football_leagues_1k:
    answer_type: open_ended
    subsets:
      - main
    format: csv
    eval_type: answers_map
    max_new_tokens: 128
    stop_words: ["<|eot_id|>", "\n"]
    answers_map_path: eval_map.json
    col_map:
      prompt: question
      answer: correct_answer
    prompts:
      base: |-
        I am going to ask you a question about football. End your sentence with <|eot_id|>.
        Here are some examples of questions that might help you:
        ---
        Question: Which team finished in position 10 in the 2010-2011 Premier League?
        Answer: Sunderland<|eot_id|> 
        ---
        Question: Which team finished in position 19 in the 2020-2021 La Liga?
        Answer: Valladolid<|eot_id|> 
        ---
        Question: Which team finished in position 13 in the 2019-2020 Serie A?
        Answer: Udinese<|eot_id|>
        ---
        Question:
  gsm8k:
    hf_repo_id: openai/gsm8k
    answer_type: open_ended
    answer_delim: "####"
    answer_regex: (?<=\$\\boxed\{)\d+(?=\}\$)
    eval_type: regex_match
    max_new_tokens: 1024
    stop_words: ["}$"]
    subsets:
      - main
    prompts:
      # TODO: replace with other n-shot examples
      base_3_shot: 
          text: |-
            Give your answer in this format: $\boxed{answer}$
            Here are some examples to help you understand the format:
            ---
            Question: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?
            Answer: $\boxed{10}$
            ---
            Question: Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?
            Answer: $\boxed{42}$
            ---
            Question: Mark has a garden with flowers. He planted plants of three different colors in it. Ten of them are yellow, and there are 80% more of those in purple. There are only 25% as many green flowers as there are yellow and purple flowers. How many flowers does Mark have in his garden?
            Answer: $\boxed{35}$
            ---
            Question:
          generation_delimiter: "Answer:"
      cot_3_shot: 
        text: |-
          Think step by step about the following problem.
          Give your answer in this format: $\boxed{answer}$
          Here are some examples to help you understand the format:
          ---
          Question: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?
          Reasoning: Weng earns 12/60 = $12/60=0.20.2 per minute. Working 50 minutes, she earned 0.2 x 50 = $0.2*50=1010.
          Final answer: $\boxed{10}$
          ---
          Question: Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?
          Reasoning: Julie read 12 x 2 = 12*2=2424 pages today. So she was able to read a total of 12 + 24 = 12+24=3636 pages since yesterday. There are 120 - 36 = 120-36=8484 pages left to be read. Since she wants to read half of the remaining pages tomorrow, then she should read 84/2 = 84/2=4242 pages.
          Final answer: $\boxed{42}$
          ---
          Question: Mark has a garden with flowers. He planted plants of three different colors in it. Ten of them are yellow, and there are 80% more of those in purple. There are only 25% as many green flowers as there are yellow and purple flowers. How many flowers does Mark have in his garden?
          Reasoning: There are 80/100 * 10 = 80/100*10=88 more purple flowers than yellow flowers. So in Mark's garden, there are 10 + 8 = 10+8=1818 purple flowers. Purple and yellow flowers sum up to 10 + 18 = 10+18=2828 flowers. That means in Mark's garden there are 25/100 * 28 = 25/100*28=77 green flowers. So in total Mark has 28 + 7 = 28+7=3535 plants in his garden.
          Final answer: $\boxed{35}$
          ---
          Question:
        generation_delimiter: "Reasoning:"


models:
  llama3.1_8b_chat:
    hf_repo_id: meta-llama/Llama-3.1-8B 
    dir_path: llama3.1_8b_chat
    eos_token: <|eot_id|>
    max_length: 1024
  llama3_3b_chat:
    hf_repo_id: meta-llama/Llama-3.2-3B 
    dir_path: llama3_3b_chat_hf
    max_length: 1024
  mistral_7b_instruct:
    hf_repo_id: mistralai/Mistral-7B-Instruct-v0.3
    dir_path: mistral_7b_instruct
    eos_token: <|eot_id|>
    max_length: 1024
  gemma_2_9b_it:
    hf_repo_id: google/gemma-2-9b-it
    dir_path: gemma_2_9b_it
    eos_token: <|eot_id|>
    max_length: 1024
  deepseek_qwen_32b:
    hf_repo_id: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
    dir_path: deepseek_qwen_32b
    eos_token: <|eot_id|>
    max_length: 1024 
  llama3.3_70b:
    hf_repo_id: meta-llama/Llama-3.3-70B
    dir_path: llama3.3_70b
    eos_token: <|eot_id|>
    max_length: 1024

format_datasets:
  raw_dir_path: raw
  formatted_dir_path: formatted
  generation_delimiter: "Answer:" # {str (e.g., "Answer:"), false}

generate_answers:
  models: 
  - llama3.1_8b_chat
  - mistral_7b_instruct
  max_dataset_size: 100 # false or integer
  sample_strategy: first_n # {first_n, random}
  batch_size: 100
  inference_engine: vllm # {vllm, hf}

capture_activations:
  raw_dir_path: raw
  batch_size: 5
  input_type: [prompt_only, prompt_answer] # {prompt_only, prompt_answer}
  layers: "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]" # must be a string due to dvc limitations


postprocess_activations:
  postprocessed_dir_path: postprocessed
  methods: [pca, tsne] # {pca, tsne}
  tsne:
    n_components: 2
    perplexity: 30
    n_iter: 1000
    random_state: 42
  pca:
    n_components: 2
    random_state: 42
  